{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "exp ='spaceloc'\n",
    "subj_list=[\"docnet1001\", \"docnet1002\"] #subs to analyze\n",
    "suf = [\"_odd\", \"_even\", \"_all\"] #runs to pull ROIs from\n",
    "rois=[\"V3ab\", \"PPC\", \"APC\", \"V4\", \"LO\", \"PFS\"] #Rois\n",
    "loc_cope=[1, 1, 1, 2, 2, 2] #copes for localizer runs; corresponding numerically to each roi\n",
    "exp_cope=[8, 9, 10, 11] #experimental copes to test in each ROI\n",
    "\n",
    "bin_size=100\n",
    "cond=[\"SA\", \"FT\", \"SS\", \"SF\"]\n",
    "cond_name =  [\"Space\", \"Feature\", \"Scrambled_Space\", \"Scrambled_Feature\"]\n",
    "\n",
    "data_dir = \"/lab_data/behrmannlab/vlad/docnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract functional localizer data from each ROI and parameter estimate from each mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(sub_dir, results_dir, lr, rr):\n",
    "    #check if ROI exists\n",
    "    for sf_loc in suf: #loop across run type for the masks\n",
    "        roi_nifti = f'{sub_dir}/rois/{lr}{rois[rr]}{sf_loc}.nii.gz'\n",
    "        if os.path.exists(roi_nifti):\n",
    "\n",
    "            #Extract functional z-stat data from the localizer\n",
    "            #if sf_loc != '_all':\n",
    "            loc_suf = f'{sf_loc}_smooth'\n",
    "            #else:\n",
    "            #   loc_suf = sf_loc\n",
    "\n",
    "            cope_nifti = f\"{sub_dir}/fsl/{exp}/HighLevel{loc_suf}.gfeat/cope{loc_cope[rr]}.feat/stats/zstat1.nii.gz\"\n",
    "            out = f'{results_dir}/{lr}{rois[rr]}{sf_loc}_localizer'\n",
    "            #extract_data(cope_nifti, roi_nifti, f'{results_dir}/{lr}{rois[rr]}{sf_loc}_localizer')\n",
    "\n",
    "            bash_cmd  = f'fslmeants -i {cope_nifti} -m {roi_nifti} -o {out}.txt --showall --transpose'\n",
    "            subprocess.run(bash_cmd.split(),check=True)\n",
    "\n",
    "            for sf_exp in suf: #loop across the run types for the copes\n",
    "                exp_suf = f'{sf_exp}_unsmoothed'\n",
    "                if sf_exp != \"_all\": #we don't test data for all\n",
    "                    for ec in range(0,len(exp_cope)):\n",
    "                        cope_nifti = f\"{sub_dir}/fsl/{exp}/HighLevel{exp_suf}.gfeat/cope{exp_cope[ec]}.feat/stats/zstat1.nii.gz\"\n",
    "                        out = f'{results_dir}/{lr}{rois[rr]}{sf_loc}{sf_exp}_{cond[ec]}'\n",
    "                        #extract_data(roi_nifti, cope_nifti, f'{results_dir}/{lr}{rois[rr]}{sf_loc}_localizer.nii.gz')\n",
    "\n",
    "                        bash_cmd  = f'fslmeants -i {cope_nifti} -m {roi_nifti} -o {out}.txt --showall --transpose'\n",
    "                        #print(bash_cmd)\n",
    "                        subprocess.run(bash_cmd.split(),check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load localizer and test data, append, average together, and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_selectivity(lr, rr, bin_size):\n",
    "    #define and read localzier files\n",
    "    odd_file = f'{results_dir}/{lr}{rois[rr]}_odd_localizer.txt'\n",
    "    even_file = f'{results_dir}/{lr}{rois[rr]}_even_localizer.txt'\n",
    "        \n",
    "    odd_df = pd.read_csv(odd_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", \"loc\"])\n",
    "    even_df = pd.read_csv(even_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", \"loc\"])\n",
    "\n",
    "\n",
    "\n",
    "    for cc in cond:\n",
    "        #define  odd and even exp file\n",
    "        #note that it should be pulled from the opposite test runs (from even ROI pull odd data)\n",
    "        #all naming convetions are relative to the ROI that data are being pulled\n",
    "        #odd_exp_file is even data pulled from *odd* run ROI\n",
    "        odd_exp_file = f'{results_dir}/{lr}{rois[rr]}_odd_even_{cc}.txt'\n",
    "        even_exp_file = f'{results_dir}/{lr}{rois[rr]}_even_odd_{cc}.txt'\n",
    "\n",
    "        #load each file\n",
    "        odd_exp = pd.read_csv(odd_exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", cc])\n",
    "        even_exp = pd.read_csv(even_exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", cc])                        \n",
    "\n",
    "        #Append it to the localizer data\n",
    "        odd_df = odd_df.join(odd_exp[cc])\n",
    "        even_df = even_df.join(even_exp[cc])\n",
    "\n",
    "    #sort file by localizer functional value (high to low)\n",
    "    odd_df = odd_df.sort_values(by =['loc'], ascending=False)\n",
    "    even_df = even_df.sort_values(by =['loc'], ascending=False)\n",
    "\n",
    "    #subsample dfs using a rolling average\n",
    "    odd_df = odd_df.rolling(bin_size, win_type='triang').mean()\n",
    "    odd_df = odd_df.dropna()\n",
    "\n",
    "    even_df = even_df.rolling(bin_size, win_type='triang').mean()\n",
    "    even_df = even_df.dropna()\n",
    "\n",
    "\n",
    "    #reduce size to smaller matrix\n",
    "    if len(odd_df) <= len(even_df):\n",
    "        even_df = even_df.head(len(odd_df))\n",
    "    else:\n",
    "        odd_df = odd_df.head(len(odd_df))\n",
    "\n",
    "    #Reset the index on both\n",
    "    odd_df= odd_df.reset_index(drop=True)\n",
    "    even_df= even_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    #average the dataframes together\n",
    "    df = pd.concat([odd_df, even_df]).groupby(level=0).mean()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate within- between- Haxby-style MVPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mvpa(lr, rr, bin_size):\n",
    "    #define and read localzier files\n",
    "    loc_file = f'{results_dir}/{lr}{rois[rr]}_all_localizer.txt'\n",
    "\n",
    "    loc_df = pd.read_csv(loc_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", \"loc\"])\n",
    "\n",
    "    for cc in cond:\n",
    "        #define  odd and even exp file\n",
    "        #note that it should be pulled from the opposite test runs (from even ROI pull odd data)\n",
    "        #all naming convetions are relative to the ROI that data are being pulled\n",
    "        #odd_exp_file is even data pulled from *odd* run ROI\n",
    "        odd_exp_file = f'{results_dir}/{lr}{rois[rr]}_all_even_{cc}.txt'\n",
    "        even_exp_file = f'{results_dir}/{lr}{rois[rr]}_all_odd_{cc}.txt'\n",
    "\n",
    "        #load each file and append localizer functional value\n",
    "        odd_exp = pd.read_csv(odd_exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", f'{cc}_odd'])\n",
    "        even_exp = pd.read_csv(even_exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", f'{cc}_even'])\n",
    "\n",
    "        loc_df = loc_df.join([odd_exp[cc + \"_odd\"],even_exp[cc+\"_even\"]])\n",
    "        #sort  by localizer value\n",
    "        loc_df = loc_df.sort_values(by =['loc'], ascending=False)\n",
    "        loc_df= loc_df.reset_index(drop=True)\n",
    "\n",
    "        #demean columns by condition\n",
    "        row_mean=loc_df.iloc[:,4:loc_df.shape[1]].mean(axis=1)\n",
    "        #loc_df.iloc[:,4:loc_df.shape[1]] =loc_df.iloc[:,4:loc_df.shape[1]].sub(row_mean,axis=0)\n",
    "        #test =loc_df.iloc[:,4:loc_df.shape[1]].sub(row_mean,axis=0)\n",
    "\n",
    "    #Start within-between analysis\n",
    "    n = 1\n",
    "    df = pd.DataFrame()\n",
    "    between_temp = pd.DataFrame() \n",
    "    for c1 in cond:\n",
    "        for c2 in cond:\n",
    "\n",
    "            temp_x = loc_df[f'{c1}_odd']\n",
    "            temp_y = loc_df[f'{c2}_even']\n",
    "            temp = temp_x.rolling(bin_size).corr(temp_y)\n",
    "            temp = temp.dropna()\n",
    "            temp= temp.reset_index(drop=True)\n",
    "\n",
    "            if c1 == c2:\n",
    "                temp = pd.DataFrame(temp)\n",
    "                temp.columns= [f'{c1}']\n",
    "                if df.empty:\n",
    "                    df = temp\n",
    "                else:\n",
    "                    df = df.join(temp)\n",
    "            else:\n",
    "                if between_temp.empty:\n",
    "                    between_temp =temp\n",
    "                else:\n",
    "                    between_temp =between_temp + temp\n",
    "                    n = n + 1\n",
    "\n",
    "    between = pd.DataFrame(between_temp/n)\n",
    "    between.columns = ['between']\n",
    "    df = df.join(between)\n",
    "\n",
    "\n",
    "    #df = df.sub(between,axis=0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make VSF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, cond, cond_name, y_ax,suf):\n",
    "    df = df[cond]\n",
    "    df.columns = cond_name\n",
    "    ax = df.plot.line()\n",
    "    ax.set_xlabel(\"Number of Voxels\")\n",
    "    ax.set_ylabel(y_ax)\n",
    "    plt.title(f'{lr}{rois[rr]}')\n",
    "    plt.savefig(f'{sub_dir}/results/{lr}{rois[rr]}_{suf}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docnet1001 r V3ab 100\n",
      "docnet1001 r PPC 100\n",
      "docnet1001 r APC 100\n",
      "docnet1001 l LO 100\n",
      "docnet1002 l V3ab 100\n",
      "docnet1002 r V3ab 100\n",
      "docnet1002 l PPC 100\n",
      "docnet1002 r PPC 100\n",
      "docnet1002 l APC 100\n",
      "docnet1002 r APC 100\n",
      "docnet1002 l V4 100\n",
      "docnet1002 r V4 100\n",
      "docnet1002 l LO 100\n",
      "docnet1002 r LO 100\n",
      "docnet1002 l PFS 100\n"
     ]
    }
   ],
   "source": [
    "for ss in subj_list:\n",
    "    sub_dir = f\"{data_dir}/sub-{ss}/ses-01/derivatives\"\n",
    "    results_dir = f'{sub_dir}/beta/selectivity'\n",
    "       \n",
    "    if os.path.exists(results_dir):\n",
    "        shutil.rmtree(results_dir)\n",
    "    \n",
    "    os.makedirs(results_dir) \n",
    "    \n",
    "\n",
    "    for rr in range(0,len(rois)): #loop across rois\n",
    "        for lr in ['l','r']: #loop across left and right hemispheres\n",
    "            odd_roi = f'{sub_dir}/rois/{lr}{rois[rr]}_odd.nii.gz'\n",
    "            even_roi = f'{sub_dir}/rois/{lr}{rois[rr]}_even.nii.gz'\n",
    "            \n",
    "            #check if odd and even roi exist\n",
    "            if os.path.exists(odd_roi) and os.path.exists(even_roi):            \n",
    "                print(ss, lr,rois[rr],bin_size)\n",
    "                extract_data(sub_dir, results_dir, lr, rr)\n",
    "                # Sort and combine split runs\n",
    "                \n",
    "                df = calc_selectivity(lr, rr, bin_size)\n",
    "                if len(df) > 2500: \n",
    "                    df = df.head(2500)\n",
    "                plot_data(df,cond,cond_name,'parameter estimate','selectivity')\n",
    "                \n",
    "                df = calc_mvpa(lr, rr, bin_size)\n",
    "                if len(df) > 2500: \n",
    "                    df = df.head(2500)\n",
    "\n",
    "                plot_data(df,cond + ['between'],cond_name + ['Between'],'Correlation','mvpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=\"docnet1001\"\n",
    "sub_dir = f\"{data_dir}/sub-{ss}/ses-01/derivatives\"\n",
    "results_dir = f'{sub_dir}/beta/selectivity'\n",
    "\n",
    "lr=\"r\"\n",
    "rr=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
