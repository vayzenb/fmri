{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "exp ='adaptation'\n",
    "subj_list=[\"docnet1001\", \"docnet1002\"] #subs to analyze\n",
    "loc_suf = [\"_all\"] #runs to pull ROIs from\n",
    "exp_suf = [\"\"]\n",
    "rois=[\"V3ab\", \"PPC\", \"APC\", \"V4\", \"LO\", \"PFS\"] #Rois\n",
    "loc_cope=[1, 1, 1, 2, 2, 2] #copes for localizer runs; corresponding numerically to each roi\n",
    "exp_cope=[1, 2, 3, 4, 5, 6] #experimental copes to test in each ROI\n",
    "\n",
    "bin_size=100\n",
    "cond=[\"AC\", \"AD\", \"AP\", \"RC\", \"RD\", \"RP\"]\n",
    "cond_name = [\"Adapt_Complete\", \"Adapt_Deleted\", \"Adapt_Perturbed\",\\\n",
    "             \"Release_Complete\", \"Release_Deleted\", \"Release_Perturbed\"]\n",
    "\n",
    "bool_extract_data = True\n",
    "bool_calc_act = False\n",
    "bool_calc_mvpa = False\n",
    "\n",
    "\n",
    "exp_dir = \"/lab_data/behrmannlab/vlad/docnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract functional localizer data from each ROI and parameter estimate from each mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(sub_dir, results_dir, lr, rr):\n",
    "    #check if ROI exists\n",
    "    for sf_loc in loc_suf: #loop across run type for the masks\n",
    "        roi_nifti = f'{sub_dir}/rois/{lr}{rois[rr]}{sf_loc}.nii.gz'\n",
    "        if os.path.exists(roi_nifti):\n",
    "\n",
    "            for sf_exp in exp_suf: #loop across the run types for the experimental copes\n",
    "                                \n",
    "                for ec in range(0,len(exp_cope)):\n",
    "                    cope_nifti = f\"{sub_dir}/fsl/{exp}/HighLevel{sf_exp}.gfeat/cope{exp_cope[ec]}.feat/stats/zstat1.nii.gz\"\n",
    "                    out = f'{results_dir}/{lr}{rois[rr]}{sf_loc}{sf_exp}_{cond[ec]}'\n",
    "                    #extract_data(roi_nifti, cope_nifti, f'{results_dir}/{lr}{rois[rr]}{sf_loc}_localizer.nii.gz')\n",
    "\n",
    "                    bash_cmd  = f'fslmeants -i {cope_nifti} -m {roi_nifti} -o {out}.txt --showall --transpose'\n",
    "                    #print(bash_cmd)\n",
    "                    bash_out = subprocess.run(bash_cmd.split(),check=True, capture_output=True, text=True)\n",
    "                    #print(bash_out.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load localizer and test data, append, average together, and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_activation(lr, rr, bin_size):\n",
    "    for sf_loc in loc_suf: #loop across run type for the masks\n",
    "        #define and read localzier files\n",
    "        loc_file = f'{roi_dir}/data/{lr}{rois[rr]}{sf_loc}.txt'\n",
    "\n",
    "        loc_df = pd.read_csv(loc_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", \"loc\"])\n",
    "\n",
    "        for sf_exp in exp_suf: #loop across experimental types\n",
    "            for cc in cond:\n",
    "                #define exp file\n",
    "                exp_file = f'{results_dir}/{lr}{rois[rr]}{sf_loc}{sf_exp}{cc}.txt'\n",
    "\n",
    "                #load each file\n",
    "                exp_df = pd.read_csv(exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", cc])\n",
    "\n",
    "                #Append it to the localizer data\n",
    "                loc_df = loc_df.join(exp_df[cc])\n",
    "\n",
    "\n",
    "            #sort file by localizer functional value (high to low)\n",
    "            loc_df = loc_df.sort_values(by =['loc'], ascending=False)\n",
    "\n",
    "            #subsample dfs using a rolling average\n",
    "            loc_df = loc_df.rolling(bin_size, win_type='triang').mean()\n",
    "            loc_df = loc_df.dropna()\n",
    "\n",
    "            #Reset the index on both\n",
    "            loc_df= loc_df.reset_index(drop=True)\n",
    "\n",
    "            return loc_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate within- between- Haxby-style MVPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mvpa(lr, rr, bin_size):\n",
    "    #define and read localzier files\n",
    "    loc_file = f'{results_dir}/{lr}{rois[rr]}_all_localizer.txt'\n",
    "\n",
    "    loc_df = pd.read_csv(loc_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", \"loc\"])\n",
    "\n",
    "    for cc in cond:\n",
    "        #define  odd and even exp file\n",
    "        #note that it should be pulled from the opposite test runs (from even ROI pull odd data)\n",
    "        #all naming convetions are relative to the ROI that data are being pulled\n",
    "        #odd_exp_file is even data pulled from *odd* run ROI\n",
    "        odd_exp_file = f'{results_dir}/{lr}{rois[rr]}_all_even_{cc}.txt'\n",
    "        even_exp_file = f'{results_dir}/{lr}{rois[rr]}_all_odd_{cc}.txt'\n",
    "\n",
    "        #load each file and append localizer functional value\n",
    "        odd_exp = pd.read_csv(odd_exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", f'{cc}_odd'])\n",
    "        even_exp = pd.read_csv(even_exp_file, sep=\"  \", header=None, names = [\"x\", \"y\", \"z\", f'{cc}_even'])\n",
    "\n",
    "        loc_df = loc_df.join([odd_exp[cc + \"_odd\"],even_exp[cc+\"_even\"]])\n",
    "        #sort  by localizer value\n",
    "        loc_df = loc_df.sort_values(by =['loc'], ascending=False)\n",
    "        loc_df= loc_df.reset_index(drop=True)\n",
    "\n",
    "        #demean columns by condition\n",
    "        row_mean=loc_df.iloc[:,4:loc_df.shape[1]].mean(axis=1)\n",
    "        #loc_df.iloc[:,4:loc_df.shape[1]] =loc_df.iloc[:,4:loc_df.shape[1]].sub(row_mean,axis=0)\n",
    "        #test =loc_df.iloc[:,4:loc_df.shape[1]].sub(row_mean,axis=0)\n",
    "\n",
    "    #Start within-between analysis\n",
    "    n = 1\n",
    "    df = pd.DataFrame()\n",
    "    between_temp = pd.DataFrame() \n",
    "    for c1 in cond:\n",
    "        for c2 in cond:\n",
    "\n",
    "            temp_x = loc_df[f'{c1}_odd']\n",
    "            temp_y = loc_df[f'{c2}_even']\n",
    "            temp = temp_x.rolling(bin_size).corr(temp_y)\n",
    "            temp = temp.dropna()\n",
    "            temp= temp.reset_index(drop=True)\n",
    "\n",
    "            if c1 == c2:\n",
    "                temp = pd.DataFrame(temp)\n",
    "                temp.columns= [f'{c1}']\n",
    "                if df.empty:\n",
    "                    df = temp\n",
    "                else:\n",
    "                    df = df.join(temp)\n",
    "            else:\n",
    "                if between_temp.empty:\n",
    "                    between_temp =temp\n",
    "                else:\n",
    "                    between_temp =between_temp + temp\n",
    "                    n = n + 1\n",
    "\n",
    "    between = pd.DataFrame(between_temp/n)\n",
    "    between.columns = ['between']\n",
    "    df = df.join(between)\n",
    "\n",
    "\n",
    "    #df = df.sub(between,axis=0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make VSF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, cond, cond_name, y_ax,suf):\n",
    "    df = df[cond]\n",
    "    df.columns = cond_name\n",
    "    ax = df.plot.line()\n",
    "    ax.set_xlabel(\"Number of Voxels\")\n",
    "    ax.set_ylabel(y_ax)\n",
    "    plt.title(f'{lr}{rois[rr]}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.savefig(f'{sub_dir}/results/{lr}{rois[rr]}_{suf}.png')\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docnet1001 r V3ab 100\n",
      "docnet1001 r PPC 100\n",
      "docnet1001 r APC 100\n",
      "docnet1001 l LO 100\n",
      "docnet1002 l V3ab 100\n",
      "docnet1002 r V3ab 100\n",
      "docnet1002 l PPC 100\n",
      "docnet1002 r PPC 100\n",
      "docnet1002 l APC 100\n",
      "docnet1002 r APC 100\n",
      "docnet1002 l V4 100\n",
      "docnet1002 r V4 100\n",
      "docnet1002 l LO 100\n",
      "docnet1002 r LO 100\n",
      "docnet1002 l PFS 100\n"
     ]
    }
   ],
   "source": [
    "for ss in subj_list:\n",
    "    sub_dir = f\"{exp_dir}/sub-{ss}/ses-01/derivatives\"\n",
    "    results_dir = f'{sub_dir}/beta/adaptation'\n",
    "    roi_dir= f'{sub_dir}/rois'\n",
    "       \n",
    "    if os.path.exists(results_dir):\n",
    "        shutil.rmtree(results_dir)\n",
    "    \n",
    "    os.makedirs(results_dir) \n",
    "    \n",
    "\n",
    "    for rr in range(0,len(rois)): #loop across rois\n",
    "        for lr in ['l','r']: #loop across left and right hemispheres\n",
    "            roi_file = f'{sub_dir}/rois/{lr}{rois[rr]}{suf}.nii.gz'\n",
    "                      \n",
    "            #check if odd and even roi exist\n",
    "            if os.path.exists(roi_file):            \n",
    "                print(ss, lr,rois[rr],bin_size)\n",
    "                \n",
    "                if bool_extract_roi == True:\n",
    "                    extract_data(sub_dir, results_dir, lr, rr)\n",
    "                \n",
    "                if bool_calc_act == True:\n",
    "                    # Sort and combine split runs                \n",
    "                    df = calc_activation(lr, rr, bin_size)\n",
    "                \n",
    "                    if len(df) > 2500: \n",
    "                        df = df.head(2500)\n",
    "                    plot_data(df, cond, cond_name, 'beta','adapt')\n",
    "                \n",
    "                if bool_calc_mvpa == True:\n",
    "                    df = calc_mvpa(lr, rr, bin_size)\n",
    "                    if len(df) > 2500: \n",
    "                        df = df.head(2500)\n",
    "\n",
    "                    plot_data(df,cond + ['between'],cond_name + ['Between'],'Correlation','mvpa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=\"docnet1001\"\n",
    "sub_dir = f\"{exp_dir}/sub-{ss}/ses-01/derivatives\"\n",
    "results_dir = f'{sub_dir}/beta/adaptation'\n",
    "roi_dir= f'{sub_dir}/rois'\n",
    "\n",
    "lr=\"r\"\n",
    "rr=2\n",
    "#extract_data(sub_dir, results_dir, lr, rr)\n",
    "\n",
    "df = calc_activation(lr, rr, bin_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
